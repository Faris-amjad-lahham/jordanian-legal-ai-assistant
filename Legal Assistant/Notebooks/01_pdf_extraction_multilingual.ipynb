{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0566b315",
   "metadata": {},
   "source": [
    "# ðŸ§  Legal Assistant â€“ Jordanian Labour Law (EN + AR) Extraction Notebook\n",
    "\n",
    "This notebook prepares **two parallel datasets** for the Jordanian Labour Law (Law No. 8 of 1996):\n",
    "\n",
    "- **English**: the official Ministry/Official Gazette English PDF.\n",
    "- **Arabic**: your **Google-Docsâ€“cleaned OCR Arabic** PDF.\n",
    "\n",
    "It extracts text, splits into articles, normalizes metadata, chunks the articles, and writes:\n",
    "- `labour_law_articles_en.json`\n",
    "- `labour_law_articles_ar.json`\n",
    "- `labour_law_articles_multilingual.json`\n",
    "- `labour_law_chunks_multilingual.json`\n",
    "- `sanity_report_multilingual.json`\n",
    "\n",
    "> âš ï¸ Disclaimer: This dataset is for informational/educational use and is **not** legal advice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16a00db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from PyPDF2 import PdfReader\n",
    "import re\n",
    "import json\n",
    "import unicodedata\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13755c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN PDF exists: True C:\\Users\\VICTUS-H\\Desktop\\Faris\\Datasets\\raw\\jordanian_labour_law_no._(8)_of_1996_and_its_amendments.pdf\n",
      "AR PDF exists: True C:\\Users\\VICTUS-H\\Desktop\\Faris\\Datasets\\raw\\(Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ø¹Ù…Ù„ Ù„Ø³Ù†Ø© 1996) .pdf\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = Path(r\"C:\\Users\\VICTUS-H\\Desktop\\Faris\\Datasets\\raw\")\n",
    "\n",
    "PDF_EN_PATH = BASE_DIR / \"jordanian_labour_law_no._(8)_of_1996_and_its_amendments.pdf\"\n",
    "PDF_AR_PATH = BASE_DIR / \"(Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ø¹Ù…Ù„ Ù„Ø³Ù†Ø© 1996) .pdf\"\n",
    "\n",
    "OUT_DIR = BASE_DIR\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"EN PDF exists:\", PDF_EN_PATH.exists(), PDF_EN_PATH)\n",
    "print(\"AR PDF exists:\", PDF_AR_PATH.exists(), PDF_AR_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09b1bd26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN TEXT LENGTH: 118963\n",
      "AR TEXT LENGTH: 104296\n",
      "\n",
      "EN preview:\n",
      " 1 \n",
      " Jordanian Labour Law No. (8) Of 1996 and its Amendmentsï€ª \n",
      " \n",
      " \n",
      "CHAPTER ONE  \n",
      "Scope of Application of this Law  and Interpretation of Terms  \n",
      " \n",
      "Article (1):  Title and Effective Date  \n",
      "This Law shall be cited as the \"The labour code for the year 1996\" and shall come \n",
      "into effect after sixty days of its publication in the Official Gazette.  \n",
      " \n",
      "Article (2):  Definitions Wherever used in this Law, and unless the context \n",
      "otherwise provides, the following terms and expressions, shall have the mean\n",
      "\n",
      "AR preview:\n",
      " Ø§ï»Ÿï»£ïºŽØ¯Ø©\n",
      " \n",
      "1\n",
      "   \n",
      "ï¯¾ïº³ï»£ï»°\n",
      " \n",
      "Ú¾Ø°Ø§\n",
      " \n",
      "Ø§ï»Ÿï»˜ïºŽï»§ÙˆÙ†\n",
      " \n",
      ")ï»—ïºŽï»§ÙˆÙ†\n",
      " \n",
      "Ø§ï»Ÿï»Œï»£Ù„\n",
      " \n",
      "ï»Ÿïº³ï»§ïº”\n",
      " \n",
      "1996\n",
      "(\n",
      " \n",
      "Ùˆï¯¾ï»Œï»£Ù„\n",
      " \n",
      "ïº‘ï®«\n",
      " \n",
      "ïº‘ï»ŒØ¯\n",
      " \n",
      "ï»£Ø±ÙˆØ±\n",
      " \n",
      "ïº³ïº—ï¯¾Ù†\n",
      " \n",
      "ï¯¾Ùˆï»£ïºŽ\n",
      " \n",
      "ï»‹ï» ï»°\n",
      " \n",
      "ïº—ïºŽØ±ï¯¾ïº¦\n",
      " \n",
      "ï»§ïº·Ø±Ù‡\n",
      " \n",
      "ï»“ï»²\n",
      " \n",
      "Ø§ï»ŸïºŸØ±ï¯¾Ø¯Ø©\n",
      " \n",
      "Ø§ï»ŸØ±ïº³ï»£ï¯¾ïº”.\n",
      " \n",
      " \n",
      " \n",
      "Ø§ï»Ÿï»£ïºŽØ¯Ø©\n",
      " \n",
      "2\n",
      " \n",
      "ï¯¾ï»›ÙˆÙ†\n",
      " \n",
      "ï»Ÿï» ï»›ï» ï»£ïºŽØª\n",
      " \n",
      "ÙˆØ§ï»Ÿï»Œïº‘ïºŽØ±Ø§Øª\n",
      " \n",
      "Ø§ï»Ÿïº—ïºŽï»Ÿï¯¾ïº”\n",
      " \n",
      "ïº£ï¯¾ïº›ï»£ïºŽ\n",
      " \n",
      "ÙˆØ±Ø¯Øª\n",
      " \n",
      "ï»“ï»²\n",
      " \n",
      "Ú¾Ø°Ø§\n",
      " \n",
      "Ø§ï»Ÿï»˜ïºŽï»§ÙˆÙ†\n",
      " \n",
      "Ø§ï»Ÿï»£ï»ŒïºŽï»§ï»²\n",
      " \n",
      "Ø§ï»Ÿï»£ïº§ïº»ïº»ïº”\n",
      " \n",
      "ï»Ÿï®­ïºŽ\n",
      " \n",
      "Ø£Ø¯ï»§ïºŽÙ‡\n",
      " \n",
      "ï»£ïºŽ\n",
      " \n",
      "ï»ŸÙ…\n",
      " \n",
      "ïº—Ø¯Ù„\n",
      " \n",
      "Ø§ï»Ÿï»˜Ø±ï¯¾ï»§ïº”\n",
      " \n",
      "ï»‹ï» ï»°\n",
      " \n",
      "ï»ï¯¾Ø±\n",
      " \n",
      "Ø°ï»ŸÙƒ:\n",
      " \n",
      " \n",
      " \n",
      "Ø§ï»ŸÙˆØ²Ø§Ø±Ø©:\n",
      " \n",
      "ÙˆØ²Ø§Ø±Ø©\n",
      " \n",
      "Ø§ï»Ÿï»Œï»£Ù„.\n",
      " \n",
      " \n",
      " \n",
      "Ø§ï»ŸÙˆØ²ï¯¾Ø±:\n",
      " \n",
      "ÙˆØ²ï¯¾Ø±\n",
      " \n",
      "Ø§ï»Ÿï»Œï»£Ù„.\n",
      " \n",
      " \n",
      " \n",
      "Ø§ï»·ï»£ï¯¾Ù†\n",
      " \n",
      "Ø§ï»Ÿï»ŒïºŽÙ…:\n",
      " \n",
      "Ø§ï»·ï»£ï¯¾Ù†\n",
      " \n",
      "Ø§ï»Ÿï»ŒïºŽÙ…\n",
      " \n",
      "ï»Ÿï» ÙˆØ²Ø§Ø±Ø©.\n",
      " \n",
      " \n",
      " \n",
      "ïº»ïºŽïº£Ø¨\n",
      " \n",
      "Ø§ï»Ÿï»Œï»£Ù„:\n",
      " \n",
      "ï»›Ù„\n",
      " \n",
      "ïº·ïº§Øµ\n",
      " \n",
      "Ø·ïº‘ï¯¾ï»Œï»²\n",
      " \n",
      "Ø£Ùˆ\n",
      " \n",
      "ï»£ï»Œï»§ÙˆÙŠ\n",
      " \n",
      "ï¯¾ïº³ïº—ïº§Ø¯Ù…\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def extract_pdf_text(pdf_path: Path) -> str:\n",
    "    reader = PdfReader(str(pdf_path))\n",
    "    parts = []\n",
    "    for page in reader.pages:\n",
    "        t = page.extract_text()\n",
    "        if t:\n",
    "            parts.append(t)\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "\n",
    "full_text_en = extract_pdf_text(PDF_EN_PATH)\n",
    "full_text_ar = extract_pdf_text(PDF_AR_PATH)\n",
    "\n",
    "print(\"EN TEXT LENGTH:\", len(full_text_en))\n",
    "print(\"AR TEXT LENGTH:\", len(full_text_ar))\n",
    "\n",
    "print(\"\\nEN preview:\\n\", full_text_en[:500])\n",
    "print(\"\\nAR preview:\\n\", full_text_ar[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e859a481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_arabic_text(text: str) -> str:\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "    arabic_digit_map = str.maketrans(\"Ù Ù¡Ù¢Ù£Ù¤Ù¥Ù¦Ù§Ù¨Ù©\", \"0123456789\")\n",
    "    text = text.translate(arabic_digit_map)\n",
    "\n",
    "    text = re.sub(r\"\\r\", \"\\n\", text)\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "    text = re.sub(r\"\\n{2,}\", \"\\n\", text)\n",
    "\n",
    "    text = re.sub(r\"(Ø§Ù„Ù…Ø§Ø¯Ø©)\\s*\\n\\s*(\\d{1,3})\", r\"\\1 \\2\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "full_text_ar = normalize_arabic_text(full_text_ar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2bf0b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN articles: 156 1 142\n",
      "AR articles: 142 1 142\n"
     ]
    }
   ],
   "source": [
    "def split_articles_en(full_text: str):\n",
    "    pattern = re.compile(\n",
    "        r\"\\bArticle\\s*\\(?\\s*(\\d{1,3})\\s*\\)?\\s*[:\\-]?\",\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    matches = list(pattern.finditer(full_text))\n",
    "    articles = []\n",
    "\n",
    "    for idx, m in enumerate(matches):\n",
    "        art_no = int(m.group(1))\n",
    "        start = m.start()\n",
    "        end = matches[idx + 1].start() if idx + 1 < len(matches) else len(full_text)\n",
    "        raw = full_text[start:end].strip()\n",
    "        raw = pattern.sub(\"\", raw, count=1).strip()\n",
    "        articles.append({\"article_number\": art_no, \"text\": raw})\n",
    "\n",
    "    return articles\n",
    "\n",
    "\n",
    "def split_articles_ar(full_text: str):\n",
    "    pattern = re.compile(\n",
    "        r\"(Ø§Ù„Ù…Ø§Ø¯Ø©\\s*(?:\\(|\\[)?\\s*(\\d{1,3})\\s*(?:\\)|\\])?)\",\n",
    "        re.UNICODE\n",
    "    )\n",
    "\n",
    "    matches = list(pattern.finditer(full_text))\n",
    "    articles = []\n",
    "\n",
    "    for idx, m in enumerate(matches):\n",
    "        art_no = int(m.group(2))\n",
    "        start = m.start()\n",
    "        end = matches[idx + 1].start() if idx + 1 < len(matches) else len(full_text)\n",
    "        raw = full_text[start:end].strip()\n",
    "        articles.append({\"article_number\": art_no, \"text\": raw})\n",
    "\n",
    "    return articles\n",
    "\n",
    "\n",
    "articles_en = split_articles_en(full_text_en)\n",
    "articles_ar = split_articles_ar(full_text_ar)\n",
    "\n",
    "print(\"EN articles:\", len(articles_en), articles_en[0][\"article_number\"], articles_en[-1][\"article_number\"])\n",
    "print(\"AR articles:\", len(articles_ar), articles_ar[0][\"article_number\"], articles_ar[-1][\"article_number\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6fcfb220",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(OUT_DIR / \"labour_law_articles_en.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(articles_en, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(OUT_DIR / \"labour_law_articles_ar.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(articles_ar, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f16770a",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_map = {a[\"article_number\"]: a[\"text\"] for a in articles_en}\n",
    "ar_map = {a[\"article_number\"]: a[\"text\"] for a in articles_ar}\n",
    "\n",
    "all_numbers = sorted(set(en_map.keys()) | set(ar_map.keys()))\n",
    "\n",
    "multilingual_articles = []\n",
    "\n",
    "for num in all_numbers:\n",
    "    multilingual_articles.append({\n",
    "        \"article_number\": num,\n",
    "        \"en\": en_map.get(num),\n",
    "        \"ar\": ar_map.get(num)\n",
    "    })\n",
    "\n",
    "with open(OUT_DIR / \"labour_law_articles_multilingual.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(multilingual_articles, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "199cc08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, max_chars=800):\n",
    "    if not text:\n",
    "        return []\n",
    "    chunks = []\n",
    "    current = \"\"\n",
    "    for line in text.split(\"\\n\"):\n",
    "        if len(current) + len(line) > max_chars:\n",
    "            chunks.append(current.strip())\n",
    "            current = \"\"\n",
    "        current += line + \"\\n\"\n",
    "    if current.strip():\n",
    "        chunks.append(current.strip())\n",
    "    return chunks\n",
    "\n",
    "\n",
    "chunks = []\n",
    "\n",
    "for art in multilingual_articles:\n",
    "    for lang in (\"en\", \"ar\"):\n",
    "        text = art.get(lang)\n",
    "        for idx, chunk in enumerate(chunk_text(text)):\n",
    "            chunks.append({\n",
    "                \"article_number\": art[\"article_number\"],\n",
    "                \"lang\": lang,\n",
    "                \"chunk_id\": idx,\n",
    "                \"text\": chunk\n",
    "            })\n",
    "\n",
    "with open(OUT_DIR / \"labour_law_chunks_multilingual.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(chunks, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a896ccb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'generated_at': '2026-02-07T01:00:38.112928',\n",
       " 'english': {'count': 156, 'min': 1, 'max': 142, 'missing': [18]},\n",
       " 'arabic': {'count': 142, 'min': 1, 'max': 142, 'missing': []}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def article_stats(articles):\n",
    "    nums = [a[\"article_number\"] for a in articles]\n",
    "    return {\n",
    "        \"count\": len(nums),\n",
    "        \"min\": min(nums) if nums else None,\n",
    "        \"max\": max(nums) if nums else None,\n",
    "        \"missing\": sorted(set(range(min(nums), max(nums)+1)) - set(nums)) if nums else []\n",
    "    }\n",
    "\n",
    "\n",
    "report = {\n",
    "    \"generated_at\": datetime.utcnow().isoformat(),\n",
    "    \"english\": article_stats(articles_en),\n",
    "    \"arabic\": article_stats(articles_ar)\n",
    "}\n",
    "\n",
    "with open(OUT_DIR / \"sanity_report_multilingual.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(report, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "51728a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Arabic cleaning complete\n",
      "â†’ labour_law_articles_ar_cleaned.json\n",
      "â†’ labour_law_articles_multilingual_cleaned.json\n",
      "â†’ labour_law_chunks_multilingual_cleaned.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "BASE_DIR = Path(r\"C:\\Users\\VICTUS-H\\Desktop\\Faris\\Datasets\\raw\")\n",
    "\n",
    "AR_INPUT_JSON = BASE_DIR / \"labour_law_articles_ar.json\"\n",
    "AR_OUTPUT_JSON = BASE_DIR / \"labour_law_articles_ar_cleaned.json\"\n",
    "MULTI_INPUT_JSON = BASE_DIR / \"labour_law_articles_multilingual.json\"\n",
    "MULTI_OUTPUT_JSON = BASE_DIR / \"labour_law_articles_multilingual_cleaned.json\"\n",
    "CHUNKS_INPUT_JSON = BASE_DIR / \"labour_law_chunks_multilingual.json\"\n",
    "CHUNKS_OUTPUT_JSON = BASE_DIR / \"labour_law_chunks_multilingual_cleaned.json\"\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Arabic Text Cleaning\n",
    "# =========================\n",
    "def clean_arabic_text(text: str) -> str:\n",
    "    if not text:\n",
    "        return text\n",
    "\n",
    "    # Unicode normalization (fix presentation forms)\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "    # Remove tatweel\n",
    "    text = re.sub(\"Ù€+\", \"\", text)\n",
    "\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r\"\\r\", \"\\n\", text)\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "    text = re.sub(r\"\\n{2,}\", \"\\n\", text)\n",
    "\n",
    "    # Merge broken OCR lines (word-per-line â†’ sentence)\n",
    "    lines = [l.strip() for l in text.split(\"\\n\") if l.strip()]\n",
    "    merged = []\n",
    "    buffer = \"\"\n",
    "\n",
    "    for line in lines:\n",
    "        # keep article headers intact\n",
    "        if line.startswith(\"Ø§Ù„Ù…Ø§Ø¯Ø©\"):\n",
    "            if buffer:\n",
    "                merged.append(buffer.strip())\n",
    "                buffer = \"\"\n",
    "            merged.append(line)\n",
    "            continue\n",
    "\n",
    "        # new paragraph if line ends with punctuation\n",
    "        if re.search(r\"[.:Ø›ØŸ!]\\s*$\", line):\n",
    "            buffer += (\" \" if buffer else \"\") + line\n",
    "            merged.append(buffer.strip())\n",
    "            buffer = \"\"\n",
    "        else:\n",
    "            buffer += (\" \" if buffer else \"\") + line\n",
    "\n",
    "    if buffer:\n",
    "        merged.append(buffer.strip())\n",
    "\n",
    "    text = \"\\n\".join(merged)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Load & Clean Arabic Articles\n",
    "# =========================\n",
    "with open(AR_INPUT_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    articles_ar = json.load(f)\n",
    "\n",
    "for art in articles_ar:\n",
    "    art[\"text\"] = clean_arabic_text(art[\"text\"])\n",
    "\n",
    "with open(AR_OUTPUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(articles_ar, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Update Multilingual Articles\n",
    "# =========================\n",
    "with open(MULTI_INPUT_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    multi_articles = json.load(f)\n",
    "\n",
    "ar_map = {a[\"article_number\"]: a[\"text\"] for a in articles_ar}\n",
    "\n",
    "for art in multi_articles:\n",
    "    if art.get(\"ar\"):\n",
    "        art[\"ar\"] = ar_map.get(art[\"article_number\"], art[\"ar\"])\n",
    "\n",
    "with open(MULTI_OUTPUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(multi_articles, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Update Chunks (Arabic Only)\n",
    "# =========================\n",
    "def chunk_text(text, max_chars=800):\n",
    "    if not text:\n",
    "        return []\n",
    "    chunks = []\n",
    "    current = \"\"\n",
    "    for line in text.split(\"\\n\"):\n",
    "        if len(current) + len(line) > max_chars:\n",
    "            chunks.append(current.strip())\n",
    "            current = \"\"\n",
    "        current += line + \" \"\n",
    "    if current.strip():\n",
    "        chunks.append(current.strip())\n",
    "    return chunks\n",
    "\n",
    "\n",
    "with open(CHUNKS_INPUT_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    chunks = json.load(f)\n",
    "\n",
    "new_chunks = []\n",
    "for ch in chunks:\n",
    "    if ch[\"lang\"] == \"ar\":\n",
    "        cleaned = clean_arabic_text(ch[\"text\"])\n",
    "        for idx, txt in enumerate(chunk_text(cleaned)):\n",
    "            new_chunks.append({\n",
    "                \"article_number\": ch[\"article_number\"],\n",
    "                \"lang\": \"ar\",\n",
    "                \"chunk_id\": idx,\n",
    "                \"text\": txt\n",
    "            })\n",
    "    else:\n",
    "        new_chunks.append(ch)\n",
    "\n",
    "with open(CHUNKS_OUTPUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(new_chunks, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "print(\"âœ… Arabic cleaning complete\")\n",
    "print(\"â†’\", AR_OUTPUT_JSON.name)\n",
    "print(\"â†’\", MULTI_OUTPUT_JSON.name)\n",
    "print(\"â†’\", CHUNKS_OUTPUT_JSON.name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
